{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/andrewmendez1/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['SPECIAL_TOKENS_ATTRIBUTES',\n '__annotations__',\n '__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__len__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_add_tokens',\n '_additional_special_tokens',\n '_batch_encode_plus',\n '_batch_prepare_for_model',\n '_bos_token',\n '_cls_token',\n '_convert_id_to_token',\n '_convert_token_to_id',\n '_convert_token_to_id_with_added_voc',\n '_encode_plus',\n '_eos_token',\n '_from_pretrained',\n '_get_padding_truncation_strategies',\n '_mask_token',\n '_pad',\n '_pad_token',\n '_pad_token_type_id',\n '_sep_token',\n '_tokenize',\n '_unk_token',\n 'add_special_tokens',\n 'add_tokens',\n 'added_tokens_decoder',\n 'added_tokens_encoder',\n 'additional_special_tokens',\n 'additional_special_tokens_ids',\n 'all_special_ids',\n 'all_special_tokens',\n 'all_special_tokens_extended',\n 'basic_tokenizer',\n 'batch_decode',\n 'batch_encode_plus',\n 'bos_token',\n 'bos_token_id',\n 'build_inputs_with_special_tokens',\n 'clean_up_tokenization',\n 'cls_token',\n 'cls_token_id',\n 'convert_ids_to_tokens',\n 'convert_tokens_to_ids',\n 'convert_tokens_to_string',\n 'create_token_type_ids_from_sequences',\n 'decode',\n 'do_basic_tokenize',\n 'encode',\n 'encode_plus',\n 'eos_token',\n 'eos_token_id',\n 'from_pretrained',\n 'get_added_vocab',\n 'get_special_tokens_mask',\n 'get_vocab',\n 'ids_to_tokens',\n 'init_inputs',\n 'init_kwargs',\n 'is_fast',\n 'mask_token',\n 'mask_token_id',\n 'max_len',\n 'max_len_sentences_pair',\n 'max_len_single_sentence',\n 'max_model_input_sizes',\n 'model_input_names',\n 'model_max_length',\n 'num_special_tokens_to_add',\n 'pad',\n 'pad_token',\n 'pad_token_id',\n 'pad_token_type_id',\n 'padding_side',\n 'prepare_for_model',\n 'prepare_for_tokenization',\n 'pretrained_init_configuration',\n 'pretrained_vocab_files_map',\n 'sanitize_special_tokens',\n 'save_pretrained',\n 'save_vocabulary',\n 'sep_token',\n 'sep_token_id',\n 'special_tokens_map',\n 'special_tokens_map_extended',\n 'tokenize',\n 'truncate_sequences',\n 'unique_no_split_tokens',\n 'unk_token',\n 'unk_token_id',\n 'verbose',\n 'vocab',\n 'vocab_files_names',\n 'vocab_size',\n 'wordpiece_tokenizer']"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\n['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
    }
   ],
   "source": [
    "# # Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "print(text)\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[101, 2040, 2001, 3958, 27227, 1029, 102, 3958, 103, 2001, 1037, 13997, 11510, 102]\n"
    }
   ],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[100]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['34324123'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596420400379",
   "display_name": "Python 3.6.10 64-bit ('gsa_eula_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}